{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing is one of the most critical step event before start thinking or using any machine learning models. A good data preprocessing can greatly improve the performence of the models. One another hand, if data is not prepared properly then the result of any model could be just \"Garbage in Garbage out\". \n",
    "\n",
    "Below are the typical steps to process a dataset:\n",
    "1. Load the dataset, in order to get a sense of the data\n",
    "2. Taking care of missing data (Optional)\n",
    "3. Encoding categorical data (Optional)\n",
    "4. Splitting dataset into the Training set and Test set (Validation set)\n",
    "5. Feature Scaling\n",
    "\n",
    "Thanks for all the powerful libararies, today we can implement above steps very easily with Python. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the libararies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **[numpy](http://www.numpy.org/)** is a popular libaray for sintific computing. Here will mainly use it's N-dimensional array object. It also has very useful linear algebra, Fourier transform, and random number capabilities\n",
    "- **[matploylib](https://matplotlib.org/)** is a Python 2D plotting library which can help us to visulize the dataset \n",
    "- **[pandas](https://pandas.pydata.org/)** is a easy-to-use data structures and data analysis tools for Python. We use it to load and separat datasets.\n",
    "- **[sklearn](http://scikit-learn.org/stable/)** is another libaray we will use later. It is a very powerful tool for data analysis. Due to its comprehensive tools we will introduce them indivdualy once we use them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Age</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Purchased</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>France</td>\n",
       "      <td>44.0</td>\n",
       "      <td>72000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spain</td>\n",
       "      <td>27.0</td>\n",
       "      <td>48000.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Germany</td>\n",
       "      <td>30.0</td>\n",
       "      <td>54000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Spain</td>\n",
       "      <td>38.0</td>\n",
       "      <td>61000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Germany</td>\n",
       "      <td>40.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>France</td>\n",
       "      <td>35.0</td>\n",
       "      <td>58000.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Spain</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>France</td>\n",
       "      <td>48.0</td>\n",
       "      <td>79000.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Germany</td>\n",
       "      <td>50.0</td>\n",
       "      <td>83000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>France</td>\n",
       "      <td>37.0</td>\n",
       "      <td>67000.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Country   Age   Salary Purchased\n",
       "0   France  44.0  72000.0        No\n",
       "1    Spain  27.0  48000.0       Yes\n",
       "2  Germany  30.0  54000.0        No\n",
       "3    Spain  38.0  61000.0        No\n",
       "4  Germany  40.0      NaN       Yes\n",
       "5   France  35.0  58000.0       Yes\n",
       "6    Spain   NaN  52000.0        No\n",
       "7   France  48.0  79000.0       Yes\n",
       "8  Germany  50.0  83000.0        No\n",
       "9   France  37.0  67000.0       Yes"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read a csv file by pandas\n",
    "dataset = pd.read_csv('Data.csv')\n",
    "# print out the loaded dataset\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# separate the dataset into X and y\n",
    "# X is independent variables. Here are columns 'Country', 'Age' and 'Salary'\n",
    "X = dataset.iloc[:, :-1].values\n",
    "# y is dependent variables. Here is column 'Purchased'\n",
    "y = dataset.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 nan]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' nan 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "# value of X\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"
     ]
    }
   ],
   "source": [
    "# value of y\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking care of missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look closely there are two missing values in the dataset. One is the age of customer 6. Another is the salary of customer 4. Most of the time we need to fullfill the missing values to make the model work. There are three main ways to do it. Using the 'mean', 'median' or 'most frequent'. Here I will implement by using the meam of each value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the sklearn libarary\n",
    "from sklearn.preprocessing import Imputer\n",
    "# Instanciate the Imputer class\n",
    "# misstion_value: the place holder for the missting value, here use the default 'NaN'\n",
    "# strategy: 'mean', 'median' and 'most_frequent'\n",
    "# aixs: 0 - impute along columns. 1 - impute along rows.\n",
    "imputer = Imputer(missing_values = 'NaN', strategy = 'mean', axis = 0)\n",
    "# Fit column Age and Salary\n",
    "imputer = imputer.fit(X[:, 1:3])\n",
    "X[:, 1:3] = imputer.transform(X[:, 1:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 63777.77777777778]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' 38.77777777777778 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "# value of X\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we used sklearn's Imputer module to help us to take care of the missing data. From the code you can see it become very easy by using the libaray. And the missing Age and Salary are filled with the mean value of their column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Encoding Categorical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In our dataset, the first column is contry name. The values in this column are text not numbers. But the machine learning model only work with numbers. So we need to encode the country name into numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 44.0 72000.0]\n",
      " [2 27.0 48000.0]\n",
      " [1 30.0 54000.0]\n",
      " [2 38.0 61000.0]\n",
      " [1 40.0 63777.77777777778]\n",
      " [0 35.0 58000.0]\n",
      " [2 38.77777777777778 52000.0]\n",
      " [0 48.0 79000.0]\n",
      " [1 50.0 83000.0]\n",
      " [0 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "# Import LabelEncoder to encode text into numbers\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Encode first column of X\n",
    "labelEncoder_X = LabelEncoder()\n",
    "X[:, 0] = labelEncoder_X.fit_transform(X[:, 0])\n",
    "\n",
    "# print value of X\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After using LabelEncoder, you can see we encode the country names from text into numbers. Here we got 'France' -> 0, 'Germany' -> 1, 'Spain' -> 2. All good, right? NO! Here's the problem, by encoding 'France', 'Germany' and 'Spain' into 0, 1 and 2, it means 'Spain' is greater than 'Germany', and 'Germany' is greater than 'France', just like 2 > 1 >0. This is wrong and all the countries should be considered on the same level. So we need to do some extra work to correct this result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.00 0.00 0.00 44.00 72000.00]\n",
      " [0.00 0.00 1.00 27.00 48000.00]\n",
      " [0.00 1.00 0.00 30.00 54000.00]\n",
      " [0.00 0.00 1.00 38.00 61000.00]\n",
      " [0.00 1.00 0.00 40.00 63777.78]\n",
      " [1.00 0.00 0.00 35.00 58000.00]\n",
      " [0.00 0.00 1.00 38.78 52000.00]\n",
      " [1.00 0.00 0.00 48.00 79000.00]\n",
      " [0.00 1.00 0.00 50.00 83000.00]\n",
      " [1.00 0.00 0.00 37.00 67000.00]]\n"
     ]
    }
   ],
   "source": [
    "# Import OneHotEncoder module\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# Instantiate OneHotEncoder and set the first column \n",
    "oneHotEncoder = OneHotEncoder(categorical_features = [0])\n",
    "# Encoder the first column of X and return an array object\n",
    "X = oneHotEncoder.fit_transform(X).toarray()\n",
    "\n",
    "# print value of X\n",
    "float_formatter = lambda x: \"%.2f\" % x\n",
    "np.set_printoptions(formatter = {'float_kind' : float_formatter})\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the categorical encoding, the first column of X is separated into three columns. Each column represent one country. By doing this the model will know which country the customer comes from and make sure the model treat all the country at the same level. Now let's encode y as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before encoding: \n",
      "['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n",
      "After encoding: \n",
      "[0 1 0 0 1 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "# before encoding\n",
    "print('Before encoding: ')\n",
    "print(y)\n",
    "\n",
    "# Since y only has two values, 'yes' and 'no', we can just simply encode the value to 1 and 0 \n",
    "labelEncoder_y = LabelEncoder()\n",
    "y = labelEncoder_y.fit_transform(y)\n",
    "\n",
    "# print y\n",
    "print('After encoding: ')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Splitting the data set into the Training set and Test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing that every machine learning process will do is to split dataset into Training set and Test set. Just like human, if the machine keep learning the same dataset, it could \"learning it by heart\". Which means that the model could perform very accurate prediction on the same dataset, but when given a new dataset it model just performs poorly. We call this kind of scenario \"overfitting\". As a result, to avoid this situation we'd like to separate the dataset into Training set which will be used for training the model. And Test set, which test if the model's performance. If the model performs poorly on the Test set then we can try to correct the settings in the model and retry it. In some cases, people even break the dataset into three portions, training set, test set and validation set. So after trained and tested, validation set can be used to verify the final performance of the model. And this method to validate the model called cross validation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: \n",
      "[[0.00 1.00 0.00 40.00 63777.78]\n",
      " [1.00 0.00 0.00 37.00 67000.00]\n",
      " [0.00 0.00 1.00 27.00 48000.00]\n",
      " [0.00 0.00 1.00 38.78 52000.00]\n",
      " [1.00 0.00 0.00 48.00 79000.00]\n",
      " [0.00 0.00 1.00 38.00 61000.00]\n",
      " [1.00 0.00 0.00 44.00 72000.00]\n",
      " [1.00 0.00 0.00 35.00 58000.00]]\n",
      "\n",
      "X_test: \n",
      "[[0.00 1.00 0.00 30.00 54000.00]\n",
      " [0.00 1.00 0.00 50.00 83000.00]]\n",
      "\n",
      "y_train: \n",
      "[1 1 1 0 1 0 0 1]\n",
      "\n",
      "y_test: \n",
      "[0 0]\n"
     ]
    }
   ],
   "source": [
    "# Import 'train_test_split', a very self explaining module\n",
    "# Please notice the libaray we use is 'model_selection' from 'sklearn'\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Split X into X_train and X_test. Split y into y_train and y_test\n",
    "# test_size: usually choose less than 0.5 of the full dataset\n",
    "# random_state: when splitting the dataset we want make the dataset to a random order first\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# Print out values\n",
    "print('X_train: ')\n",
    "print(X_train)\n",
    "print()\n",
    "print('X_test: ')\n",
    "print(X_test)\n",
    "print()\n",
    "print('y_train: ')\n",
    "print(y_train)\n",
    "print()\n",
    "print('y_test: ')\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Feature Scaling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature scaling is another must do step for most of the data preprocessing. What it dose is to scale the values in a dataset into a range of -1 ~ 1. To do this has two benefits, firt the computation time is shorter with small scale numbers. Second, is to avoid a certain feature dominate the result due to a bigger scale. For example, in our dataset there are Age and Salary features. The values of Salary are much bigger than Age, so during the training process Salary feature has a chance to dominate the result and make Age feature become useless. So by scale them into the same level, we can avoid this problem to happen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two main ways to scale the feature: Standardization and Normalization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardization: $$x' = \\frac{x - mean(X)}{standard_-deviation_-(x)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization : $$x' = \\frac{x - mean(x)}{max(x) - min(x)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: \n",
      "[[0.00 1.00 0.00 0.26 0.12]\n",
      " [1.00 0.00 0.00 -0.25 0.46]\n",
      " [0.00 0.00 1.00 -1.98 -1.53]\n",
      " [0.00 0.00 1.00 0.05 -1.11]\n",
      " [1.00 0.00 0.00 1.64 1.72]\n",
      " [0.00 0.00 1.00 -0.08 -0.17]\n",
      " [1.00 0.00 0.00 0.95 0.99]\n",
      " [1.00 0.00 0.00 -0.60 -0.48]]\n",
      "\n",
      "X_test: \n",
      "[[0.00 1.00 0.00 -1.46 -0.90]\n",
      " [0.00 1.00 0.00 1.98 2.14]]\n"
     ]
    }
   ],
   "source": [
    "# Here I use Standardization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "X_train[:, 3:5] = sc_X.fit_transform(X_train[:, 3:5])\n",
    "X_test[:, 3:5] = sc_X.transform(X_test[:, 3:5])\n",
    "\n",
    "# Print out X_train, X_test\n",
    "print('X_train: ')\n",
    "print(X_train)\n",
    "print()\n",
    "print('X_test: ')\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we have all the step implemented for data preprocessing. In practice, not all the steps are needed. Please select the required steps based on your dataset. Below is the full version of the code.\n",
    "\n",
    "``` python\n",
    "# Data Preprocessing\n",
    "\n",
    "# Import the libararies\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Import the dataset\n",
    "dataset = pd.read_csv('Data.csv')\n",
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, -1].values\n",
    "\n",
    "# Taking care of missing data\n",
    "from sklearn.preprocessing import Imputer\n",
    "imputer = Imputer(missing_values = 'NaN', strategy = 'mean', axis = 0)\n",
    "imputer = imputer.fit(X[:, 1:3])\n",
    "X[:, 1:3] = imputer.transform(X[:, 1:3])\n",
    "\n",
    "# Encoding Categorical Data\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelEncoder_X = LabelEncoder()\n",
    "X[:, 0] = labelEncoder_X.fit_transform(X[:, 0])\n",
    "onehotencoder = OneHotEncoder(categorical_features = [0])\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "labelEncoder_y = LabelEncoder()\n",
    "y = labelEncoder_y.fit_transform(y)\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
